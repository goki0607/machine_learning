{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI-UA 0473 - Introduction to Machine Learning\n",
    "## Homework 4\n",
    "### Assignment 4 - Adaptive RBFN and Stochastic Gradient Descent (25 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task for this homework is again that of handwritten digit recognition on the MNIST dataset.  **For this time, you will train on the full dataset (that is, 60000 training images plus 10000 test images), rather than a small subset.**\n",
    "\n",
    "Your first goal is to implement an adaptive radial basis function network, and train it with stochastic gradient descent (SGD), while going over a few questions.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "**Please do not miss any text with red background; they are either questions you need to answer, or directions to implement some functionality.**\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "**Please start this homework early.**  Stochastic gradient descent will take a particularly long time to train.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "**Please submit a fully-run notebook; not doing so would incur at least 10% penalty problem-wise.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd.scipy as ascipy\n",
    "import autograd.misc.flatten\n",
    "from autograd import value_and_grad\n",
    "import scipy.optimize\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npz = np.load('mnist.npz')\n",
    "# Labels\n",
    "train_labels = npz['train_labels']\n",
    "test_labels = npz['test_labels']\n",
    "# Data.  We linearly scale the integers into something between 0 and 1.\n",
    "train_data = npz['train_data'] / 255.\n",
    "test_data = npz['test_data'] / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes and types for each numpy array is:\n",
    "\n",
    "* `train_data`: 60000x28x28 float64 array\n",
    "* `train_labels`: 60000 int64 array\n",
    "* `test_data`: 10000x28x28 float64 array\n",
    "* `test_labels`: 10000 int64 array\n",
    "\n",
    "The digit for i-th training data (`train_data[i]`) is `train_labels[i]`.  The same holds for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1.05,'Digit 5')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.matshow(train_data[0], vmin=0, vmax=1, cmap='gray')\n",
    "# See help(str.format) or https://docs.python.org/3.1/library/string.html#formatspec for usage of the Pythonic formatter\n",
    "# If you are comfortable with C you can also write things like sprintf(): 'Digit %d' % (train_labels[0])\n",
    "plot.title('Digit {}'.format(train_labels[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definition\n",
    "\n",
    "The first thing we need is model definition.  Recall that the Gaussian Radial Basis Function we have learned in the class transforms an arbitrary input vector $\\mathbf{x} \\in \\mathbb{R}^d$ into another vector $\\phi(\\mathbf{x}) \\in \\mathbb{R}^k$ based on a set of bases $\\mathbf{r}_k$:\n",
    "\n",
    "$$\n",
    "\\phi(\\mathbf{x})_k = \\exp\\left(\\lVert \\mathbf{x} - \\mathbf{r}_k \\rVert^2\\right) = \\exp \\left( - \\sum_{i=1}^d (x_i - r_{ki})^2\\right)\n",
    "$$\n",
    "\n",
    "An alternative would be taking an average of the squares instead of summing them up:\n",
    "\n",
    "$$\n",
    "\\phi(\\mathbf{x})_k = \\exp\\left(\\lVert \\mathbf{x} - \\mathbf{r}_k \\rVert^2\\right) = \\exp \\left( - \\dfrac{1}{d} \\sum_{i=1}^d (x_i - r_{ki})^2\\right)\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "(5 points) Between the two options, which one do you think that would work better?  Justify your answer and replace the `XXX` in the `adaptive_rbfn_logits` function with your choice.  **Note: \"by experiment\" is not a sufficient justification for *this* problem.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The use of the average of the squares works better as it prevents an outlier basis such as as a very slanted or \"unorthodox\" digit from skewing the results for $\\phi(\\textbf{x})_{k}$. We can measure how on average an input is close to a basis digit compared to how close in total an input is close to a basis digit which may be susceptible to skewing because of outlier entries in either the input vector or the basis vectors. Instead, the average will better capture relationships between the input vector and the basis vectors meaning it will work better. This observation is backed up empirically as the trained models below perform better with the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adaptive_rbfn_logits(x, params):\n",
    "    '''\n",
    "    x: numpy array of training data, with shape (n_samples, 28, 28)\n",
    "    params: a dictionary with \"bases\", \"w\" and \"b\" as keys.\n",
    "    '''\n",
    "    bases = params['bases']\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    \n",
    "    #### BEGINNING OF IMPLEMENTATION OF REPLACEMENT\n",
    "    phi = np.exp(-((x[:, None, :] - bases[None, :, :]) ** 2).mean(axis=-1)) # (n_samples, n_bases)\n",
    "    #### END OF IMPLEMENTATION OF REPLACEMENT\n",
    "    \n",
    "    #### BEGINNING OF PREVIOUS CODE FOR REFERENCE PURPOSES\n",
    "    ## phi = np.exp(-((x[:, None, :] - bases[None, :, :]) ** 2).XXX(axis=-1)) # (n_samples, n_bases)\n",
    "    #### END OF PREVIOUS CODE FOR REFERENCE PURPOSES\n",
    "    \n",
    "    logits = np.dot(phi, w) + b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above applies a linear transformation on $\\phi(\\mathbf{x})$ to get the *logits* for each class.  The logits can be considered as \"scores\"; the higher the logit, the more likely the class.  In fact, the probability of a certain class can be directly computed by applying a *softmax* on the logits.\n",
    "\n",
    "Recall that the multiclass logistic regression minimizes the negative log-likelihood (sometimes also called *cross entropy*) of the entire dataset:\n",
    "\n",
    "$$\n",
    "J(M, D_\\text{tra}) = -\\dfrac{1}{N} \\sum_{(\\mathbf{x}, y^*) \\in D_\\text{tra}}\n",
    "\\log \\dfrac{\\exp \\mathbf{w}^T_{y^*} \\tilde{\\mathbf{x}}}{\\sum_y \\exp \\mathbf{w}^T_y \\tilde{\\mathbf{x}}}\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "(5 points) The following function implements the cost function in a naive, numerically unstable way.  Please rewrite the indicated block **in at most 2 lines** so that it becomes numerically stable.  You will only get 2 points if your replacement is correct and more than 2 lines.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Hint:** Recall Problem 2 in Homework 3, then look at `autograd.scipy.misc`, then see how we select elements efficiently (which is usually called [fancy](https://docs.scipy.org/doc/numpy/user/basics.indexing.html#index-arrays) [indexing](https://docs.scipy.org/doc/numpy/user/basics.indexing.html#indexing-multi-dimensional-arrays)). <br/>\n",
    "\n",
    "Other autodifferentiation frameworks also have their numerically stable log-softmax functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adaptive_rbfn_cost(param_buf, x, y, unpack):\n",
    "    '''\n",
    "    The cost function for ARBFN.\n",
    "    x: numpy array of training data, with shape (n_samples, 28, 28)\n",
    "    y: numpy array of training labels, with shape (n_samples,).  All elements are integers.\n",
    "    param_buf: the *flattened* numpy array containing all the parameters.\n",
    "    unpack: a function that recovers the flattened parameters into the original list/dict.\n",
    "    \n",
    "    param_buf and unpack are quite a nuisance as they are introduced by autograd.\n",
    "    A more detailed explanation comes with the \"autograd.misc.flatten\" function below.\n",
    "    '''\n",
    "    params = unpack(param_buf)\n",
    "    logits = adaptive_rbfn_logits(x, params)\n",
    "    \n",
    "    #### BEGINNING OF IMPLEMENTATION OF REPLACEMENT\n",
    "    log_probs = logits.ravel()[[i*logits.shape[1]+j for i, j in enumerate(y)]] - autograd.scipy.misc.logsumexp(logits, axis=1)\n",
    "    \n",
    "    ## -- this softmax is used to be with its corresponding cost function that can be\n",
    "    ## -- seen below, I kept this here for reference purposes\n",
    "    ## (1) softmax = logits - autograd.scipy.misc.logsumexp(logits, axis=1, keepdims=True)\n",
    "    ## -- the cost below is second take at indexing, I found an even more optimized way to\n",
    "    ## -- make this computation above (omitting some uneccessary subtractions)\n",
    "    ## (1) cost = -(softmax.ravel()[[i*softmax.shape[1]+j for i, j in enumerate(y)]].mean())\n",
    "    \n",
    "    ## -- the cost below is first take at indexing, this proved to be considerably slow\n",
    "    ## -- so it was omitted from the implementation and optimized above, it can be used\n",
    "    ## -- with the softmax given in (1)\n",
    "    ## (2) cost = -(softmax[np.arange(x.shape[0]), y].mean())\n",
    "    \n",
    "    cost = -log_probs.mean()\n",
    "    #### END OF IMPLEMENTATION OF REPLACEMENT\n",
    "    \n",
    "    #### BEGINNING OF CODE TO BE REPLACED\n",
    "    ## exp_logits = np.exp(logits)\n",
    "    ## softmax = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n",
    "    ## log_probs = np.log(softmax[np.arange(x.shape[0]), y])\n",
    "    ## cost = -log_probs.mean()\n",
    "    #### END OF CODE TO BE REPLACED\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This statement returns a function that has the same signature as adaptive_rbfn_cost(),\n",
    "# but returns both the cost and the gradient.\n",
    "adaptive_rbfn_cost_and_grad = value_and_grad(adaptive_rbfn_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_full_batch(cost_and_grad, initial_params, train_data, train_labels, use_scipy=True, learning_rate=1, max_iter=30000):\n",
    "    # NOTE: We should have early stopping here, but it is left for you to implement in the next problem (not here).\n",
    "    # Autograd.misc.flatten() is a tricky function that allows us to compute the gradients to all parameters\n",
    "    # with a single grad() call.\n",
    "    # It takes in a list or dictionary, and returns two objects:\n",
    "    # (1) a flattened, i.e. 1D, numpy array which contains all the parameters, and\n",
    "    # (2) an unflatten function, which will recover the original list/dictionary structure from the flattened array.\n",
    "    # Collapsing all the parameters into a single array would also allow us to use scipy's optimize function which\n",
    "    # only takes one input variable.\n",
    "    param_buf, unpack = autograd.misc.flatten(initial_params)\n",
    "\n",
    "    if not use_scipy:\n",
    "        for i in range(max_iter):\n",
    "            loss, dparam_buf = cost_and_grad(param_buf, train_data, train_labels, unpack)\n",
    "            param_buf -= learning_rate * dparam_buf\n",
    "            if i % 100 == 0:\n",
    "                print(i, loss)\n",
    "    else:\n",
    "        model = scipy.optimize.minimize(cost_and_grad, param_buf, (train_data, train_labels, unpack), jac=True, method='CG')\n",
    "        print(model)\n",
    "        param_buf = model.x\n",
    "    \n",
    "    return unpack(param_buf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined a function for training an adaptive RBFN, let's see how a small model works on a small dataset (100 training samples, 10 test samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = 100\n",
    "test_size = 10\n",
    "n_bases = 10\n",
    "n_features = np.prod(train_data.shape[1:])\n",
    "\n",
    "params = {}\n",
    "params['bases'] = np.random.rand(n_bases, n_features)\n",
    "params['w'] = np.random.randn(n_bases, 10) * 0.1\n",
    "params['b'] = np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.331095497814928\n",
      "100 2.2623888489187562\n",
      "200 2.261649766087085\n",
      "300 2.2608647401008137\n",
      "400 2.2600032503226957\n",
      "500 2.2590298240994784\n",
      "600 2.257901701492925\n",
      "700 2.256565980954817\n",
      "800 2.254956067730879\n",
      "900 2.252987232605562\n",
      "1000 2.250551092399784\n",
      "1100 2.2475088701838404\n",
      "1200 2.243683425524489\n",
      "1300 2.238850333321621\n",
      "1400 2.232728838017366\n",
      "1500 2.2249744509425953\n",
      "1600 2.215176410346268\n",
      "1700 2.202865158928378\n",
      "1800 2.187536953154281\n",
      "1900 2.168703342475391\n",
      "2000 2.1459698635612914\n",
      "2100 2.119137403570674\n",
      "2200 2.0883002015621357\n",
      "2300 2.0538941931771566\n",
      "2400 2.0166488870051613\n",
      "2500 1.9774346199258042\n",
      "2600 1.937060816979696\n",
      "2700 1.8961158423080386\n",
      "2800 1.8549083984756667\n",
      "2900 1.8135052889678613\n",
      "3000 1.7718192076942372\n",
      "3100 1.7297013883153192\n",
      "3200 1.6870154020154045\n",
      "3300 1.6436867404810545\n",
      "3400 1.5997309498031496\n",
      "3500 1.5552641336495268\n",
      "3600 1.5104983829477883\n",
      "3700 1.4657240651309371\n",
      "3800 1.4212817915685059\n",
      "3900 1.3775286390101025\n",
      "4000 1.3348044947336473\n",
      "4100 1.2934041612719889\n",
      "4200 1.2535589911704301\n",
      "4300 1.2154291419420622\n",
      "4400 1.1791051379173303\n",
      "4500 1.1446160109783858\n",
      "4600 1.111940974989743\n",
      "4700 1.0810220533631298\n",
      "4800 1.0517758886678816\n",
      "4900 1.0241037727775022\n",
      "5000 0.9978995638118296\n",
      "5100 0.9730555569401232\n",
      "5200 0.94946658262735\n",
      "5300 0.9270326775005155\n",
      "5400 0.9056606657874601\n",
      "5500 0.8852649445335237\n",
      "5600 0.8657677092587295\n",
      "5700 0.8470988021103352\n",
      "5800 0.8291953178458512\n",
      "5900 0.812001065695746\n",
      "6000 0.7954659566846936\n",
      "6100 0.7795453648703825\n",
      "6200 0.7641994955708598\n",
      "6300 0.7493927825407275\n",
      "6400 0.7350933280459412\n",
      "6500 0.721272393986302\n",
      "6600 0.7079039479848314\n",
      "6700 0.6949642652642402\n",
      "6800 0.6824315848694167\n",
      "6900 0.6702858171785588\n",
      "7000 0.6585082985450866\n",
      "7100 0.647081588235253\n",
      "7200 0.6359893024987032\n",
      "7300 0.6252159805659057\n",
      "7400 0.614746977546232\n",
      "7500 0.6045683795453201\n",
      "7600 0.594666936776259\n",
      "7700 0.5850300109579873\n",
      "7800 0.5756455338355064\n",
      "7900 0.5665019741878514\n",
      "8000 0.557588311187568\n",
      "8100 0.5488940124240159\n",
      "8200 0.540409015293461\n",
      "8300 0.5321237107885073\n",
      "8400 0.524028928989144\n",
      "8500 0.5161159257716497\n",
      "8600 0.5083763704156767\n",
      "8700 0.5008023339108951\n",
      "8800 0.4933862778495453\n",
      "8900 0.48612104384693194\n",
      "9000 0.47899984346439645\n",
      "9100 0.47201624862413766\n",
      "9200 0.4651641825070209\n",
      "9300 0.4584379109171936\n",
      "9400 0.45183203408407313\n",
      "9500 0.44534147885573705\n",
      "9600 0.43896149121997013\n",
      "9700 0.43268762907189084\n",
      "9800 0.4265157551314559\n",
      "9900 0.4204420299012643\n",
      "10000 0.41446290454569334\n",
      "10100 0.4085751135671059\n",
      "10200 0.4027756671540189\n",
      "10300 0.39706184307999337\n",
      "10400 0.39143117804060784\n",
      "10500 0.3858814583291446\n",
      "10600 0.38041070976924707\n",
      "10700 0.3750171868443605\n",
      "10800 0.3696993609886263\n",
      "10900 0.3644559080312692\n",
      "11000 0.3592856948154997\n",
      "11100 0.3541877650425\n",
      "11200 0.3491613244200742\n",
      "11300 0.3442057252229285\n",
      "11400 0.33932045039617903\n",
      "11500 0.33450509735460243\n",
      "11600 0.3297593616464873\n",
      "11700 0.3250830206620608\n",
      "11800 0.32047591757192007\n",
      "11900 0.3159379456805471\n",
      "12000 0.31146903337388876\n",
      "12100 0.3070691298285283\n",
      "12200 0.3027381916336832\n",
      "12300 0.2984761704569513\n",
      "12400 0.294283001861266\n",
      "12500 0.29015859535490146\n",
      "12600 0.286102825729645\n",
      "12700 0.28211552571537296\n",
      "12800 0.2781964799532087\n",
      "12900 0.27434542026498265\n",
      "13000 0.2705620221745548\n",
      "13100 0.26684590261721136\n",
      "13200 0.26319661875717887\n",
      "13300 0.25961366782048784\n",
      "13400 0.25609648784103906\n",
      "13500 0.25264445921166784\n",
      "13600 0.24925690692909408\n",
      "13700 0.24593310342158042\n",
      "13800 0.24267227185057877\n",
      "13900 0.23947358978221864\n",
      "14000 0.2363361931307838\n",
      "14100 0.2332591802839386\n",
      "14200 0.23024161632800663\n",
      "14300 0.227282537300712\n",
      "14400 0.22438095440815803\n",
      "14500 0.22153585815214846\n",
      "14600 0.21874622232301666\n",
      "14700 0.21601100782175742\n",
      "14800 0.2133291662832703\n",
      "14900 0.21069964347987125\n",
      "15000 0.20812138249080014\n",
      "15100 0.2055933266292608\n",
      "15200 0.20311442212354106\n",
      "15300 0.20068362055300704\n",
      "15400 0.19829988104330243\n",
      "15500 0.1959621722279174\n",
      "15600 0.19366947398553702\n",
      "15700 0.19142077896425316\n",
      "15800 0.18921509390492086\n",
      "15900 0.18705144077671268\n",
      "16000 0.18492885773834455\n",
      "16100 0.18284639993856333\n",
      "16200 0.18080314016936272\n",
      "16300 0.17879816938507978\n",
      "16400 0.17683059710005422\n",
      "16500 0.1748995516769551\n",
      "16600 0.1730041805172231\n",
      "16700 0.17114365016436814\n",
      "16800 0.1693171463301255\n",
      "16900 0.16752387385272877\n",
      "17000 0.165763056595817\n",
      "17100 0.164033937295777\n",
      "17200 0.16233577736461538\n",
      "17300 0.160667856654802\n",
      "17400 0.159029473191893\n",
      "17500 0.15741994288015793\n",
      "17600 0.1558385991858865\n",
      "17700 0.1542847928025477\n",
      "17800 0.1527578913015073\n",
      "17900 0.15125727877158596\n",
      "18000 0.14978235545035118\n",
      "18100 0.14833253734968568\n",
      "18200 0.14690725587785544\n",
      "18300 0.1455059574600139\n",
      "18400 0.14412810315882368\n",
      "18500 0.14277316829664038\n",
      "18600 0.1414406420805034\n",
      "18700 0.14013002723099072\n",
      "18800 0.13884083961583088\n",
      "18900 0.13757260788902392\n",
      "19000 0.13632487313609\n",
      "19100 0.1350971885259564\n",
      "19200 0.13388911896988998\n",
      "19300 0.1327002407877964\n",
      "19400 0.13153014138213215\n",
      "19500 0.13037841891960608\n",
      "19600 0.12924468202079126\n",
      "19700 0.12812854945771665\n",
      "19800 0.12702964985946552\n",
      "19900 0.12594762142576854\n",
      "20000 0.1248821116485497\n",
      "20100 0.12383277704135338\n",
      "20200 0.1227992828765622\n",
      "20300 0.12178130293029014\n",
      "20400 0.12077851923482473\n",
      "20500 0.11979062183847726\n",
      "20600 0.11881730857268774\n",
      "20700 0.11785828482622493\n",
      "20800 0.11691326332631637\n",
      "20900 0.1159819639265335\n",
      "21000 0.11506411340126248\n",
      "21100 0.11415944524657853\n",
      "21200 0.11326769948734956\n",
      "21300 0.11238862249039201\n",
      "21400 0.11152196678349782\n",
      "21500 0.1106674908801633\n",
      "21600 0.10982495910984076\n",
      "21700 0.10899414145354584\n",
      "21800 0.10817481338465186\n",
      "21900 0.10736675571470423\n",
      "22000 0.10656975444409719\n",
      "22100 0.10578360061745296\n",
      "22200 0.10500809018354967\n",
      "22300 0.10424302385965097\n",
      "22400 0.10348820700008651\n",
      "22500 0.10274344946894783\n",
      "22600 0.10200856551675792\n",
      "22700 0.10128337366098286\n",
      "22800 0.10056769657025519\n",
      "22900 0.09986136095218616\n",
      "23000 0.09916419744464214\n",
      "23100 0.0984760405103729\n",
      "23200 0.0977967283348728\n",
      "23300 0.09712610272737038\n",
      "23400 0.09646400902483744\n",
      "23500 0.0958102959989175\n",
      "23600 0.0951648157656734\n",
      "23700 0.09452742369805932\n",
      "23800 0.09389797834102807\n",
      "23900 0.09327634132917945\n",
      "24000 0.09266237730687038\n",
      "24100 0.09205595385070023\n",
      "24200 0.09145694139429425\n",
      "24300 0.0908652131553071\n",
      "24400 0.09028064506457366\n",
      "24500 0.08970311569733531\n",
      "24600 0.08913250620647364\n",
      "24700 0.08856870025768444\n",
      "24800 0.08801158396653047\n",
      "24900 0.08746104583730802\n",
      "25000 0.08691697670367154\n",
      "25100 0.0863792696709578\n",
      "25200 0.08584782006015321\n",
      "25300 0.08532252535345336\n",
      "25400 0.08480328514136307\n",
      "25500 0.08429000107128629\n",
      "25600 0.08378257679756022\n",
      "25700 0.08328091793288543\n",
      "25800 0.08278493200111042\n",
      "25900 0.08229452839132573\n",
      "26000 0.08180961831322747\n",
      "26100 0.0813301147537096\n",
      "26200 0.08085593243464817\n",
      "26300 0.08038698777183892\n",
      "26400 0.07992319883505317\n",
      "26500 0.07946448530917752\n",
      "26600 0.07901076845640442\n",
      "26700 0.0785619710794402\n",
      "26800 0.07811801748570157\n",
      "26900 0.07767883345246751\n",
      "27000 0.07724434619296178\n",
      "27100 0.07681448432333376\n",
      "27200 0.07638917783051397\n",
      "27300 0.07596835804091555\n",
      "27400 0.07555195758995864\n",
      "27500 0.07513991039239225\n",
      "27600 0.07473215161338864\n",
      "27700 0.07432861764039073\n",
      "27800 0.07392924605568715\n",
      "27900 0.07353397560969605\n",
      "28000 0.07314274619493467\n",
      "28100 0.07275549882065714\n",
      "28200 0.07237217558814014\n",
      "28300 0.07199271966659664\n",
      "28400 0.07161707526970179\n",
      "28500 0.07124518763271209\n",
      "28600 0.0708770029901605\n",
      "28700 0.07051246855411267\n",
      "28800 0.07015153249296714\n",
      "28900 0.06979414391078441\n",
      "29000 0.06944025282713064\n",
      "29100 0.06908981015742\n",
      "29200 0.06874276769374371\n",
      "29300 0.06839907808617122\n",
      "29400 0.06805869482450908\n",
      "29500 0.06772157222050867\n",
      "29600 0.06738766539050534\n",
      "29700 0.06705693023848162\n",
      "29800 0.0667293234395398\n",
      "29900 0.0664048024237745\n",
      "CPU times: user 16min 31s, sys: 1min 32s, total: 18min 3s\n",
      "Wall time: 18min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_params = train_full_batch(adaptive_rbfn_cost_and_grad, params, train_data[:train_size], train_labels[:train_size], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 / 10\n"
     ]
    }
   ],
   "source": [
    "y_hat = adaptive_rbfn_logits(test_data[:test_size], new_params).argmax(axis=1)\n",
    "print(np.equal(y_hat, test_labels[:test_size]).sum(), '/', test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the gradient is computed from the entire training set, we usually call it *(full) batch gradient descent*.  However, batch gradient descent is time- and memory-consuming, in the sense that it has to go through all the training examples and average all the gradients for a *single* step, and it quickly becomes impractical if the dataset or the model becomes large.\n",
    "\n",
    "The solution is *not* to see all the examples for a single gradient step.  Instead, for each step, we compute and average the gradients for only a small subset (commonly called a *minibatch*) of examples.  This is called *minibatch gradient descent*, or *stochastic gradient descent* (SGD), though the latter refer to the special case where the minibatch only contains a single example in some literatures.\n",
    "\n",
    "The intuition behind SGD is that the minibatch gradient is a noisy but unbiased estimate of the true gradient.  This of course has pros and cons.\n",
    "\n",
    "The main downside is that, due to the inherent noise of minibatch gradient, SGD no longer guarantees to converge to a local minimum, and sometimes it will fluctuates or even overshoot itself.  You will have to use a significantly smaller, and sometimes decaying, learning rate.  Very often, you would see a curve of training loss like [this](http://cs231n.github.io/neural-networks-3/#loss):\n",
    "\n",
    "![](http://cs231n.github.io/assets/nn3/loss.jpeg)\n",
    "\n",
    "The obvious upside is that estimating the gradient using minibatch is much more efficient than computing the true gradient across all training examples.  Or, after we iterate over the entire training set, we had already performed a lot of gradient steps, which is not necessarily a bad thing.  In fact, the size of a minibatch itself can be considered as a hyperparameter to fiddle with ~~although in practice few people do that~~.\n",
    "\n",
    "In practice, SGD often works good enough, especially for large models (such as neural networks) on large datasets.\n",
    "\n",
    "See [Leon Bottou's SGD Tricks](https://cilvr.cs.nyu.edu/diglib/lsml/bottou-sgd-tricks-2012.pdf) if you are interested in the details of SGD.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "(15 points) Implement SGD, and train your ARBFN model with 20 bases (or any number you like) on the whole dataset, with the size of minibatch being 10.  <br/>\n",
    "Plot the training loss and validation accuracy, and report the test accuracy.  **Your implementation should have a test accuracy above 80% to be considered correct.** <br/>\n",
    "Do the same for minibatch size 100 and answer this question in the cell below: *What do you observe from the learning curve and training time of different minibatch sizes?*  <br/>\n",
    "\n",
    "You are free to define your own `train_sgd` function.  **Please do not use `autograd.misc.optimizers` package.**\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Hint:** To save your time figuring out the complete workflow of SGD, here is how we usually do it.\n",
    "<ol>\n",
    "    <li> Split the training set into a validation set and another smaller training set. </li>\n",
    "    <li> Shuffle the (rest of the) training set.  Of course, you can shuffle the indices rather than the actual items. </li>\n",
    "    <li> Divide the training set into equally-sized minibatches.  The last batch may not have the same size as the others, and it's up to you how to deal with it (usually it doesn't matter too much). </li>\n",
    "    <li> Iterate over each minibatch, compute the gradient, and update the parameters. </li>\n",
    "    <li> After going through all the minibatches (we call that *an epoch*), evaluate the model on validation set, and decide whether to early stop.  Since SGD will fluctuate a lot, you may want to leave it run for a little while longer even if the validation error does not seem to decrease.  </li>\n",
    "    <li> Repeat 2-5 until done. </li>\n",
    "</ol>\n",
    "\n",
    "Optionally, you can also decay the learning rate after each minibatch or each epoch.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "**Warning:** This assignment would take potentially hours to train a model to \"good performance\" on an average laptop.  In practice, it is very common for neural networks to take hours, days or even weeks to train, even if powered by GPUs which are good at matrix multiplication and elementwise computation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** For my run when the minibatch size is increased from $10$ to $100$ then training time increases. This implies that increasing batch sizes in general should increase the training time. Conversely, increasing the batch size from $10$ to $100$ does not change the validation accuracy by a lot but has led to an increase of the testing accuracy from 0.9042 to 0.9177. In both cases the learning curves look the same for both minibatch sizes but the validation accuracy curve is much smoother when the minibatch size is $100$. Minibatch size $10$ takes 134 epochs until early stopping while minibatch size $100$ takes 245 epochs until early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# import shuffle for shuffling multiple datasets\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# define training set size and validation set size\n",
    "# we use 50,000 data points for training\n",
    "train_size = 50000\n",
    "# we use 10,000 data points for validation\n",
    "val_size = 10000\n",
    "\n",
    "# partition the sets for use\n",
    "X_train, X_val = train_data[:train_size], train_data[-val_size:]\n",
    "y_train, y_val = train_labels[:train_size], train_labels[-val_size:]\n",
    "\n",
    "# the number of bases is 20 (as specified above)\n",
    "n_bases = 20\n",
    "n_features = np.prod(X_train.shape[1:])\n",
    "\n",
    "# create params (parameters)\n",
    "params = {}\n",
    "params['bases'] = np.random.rand(n_bases, n_features)\n",
    "params['w'] = np.random.randn(n_bases, 10) * 0.1\n",
    "params['b'] = np.zeros(10)\n",
    "\n",
    "# get both cost and gradient of adaptive_rbfn_cost\n",
    "adaptive_rbfn_cost_and_grad = value_and_grad(adaptive_rbfn_cost)\n",
    "\n",
    "# train_sgd function, accepts as input:\n",
    "#   cost_and_grad    -- autograd computed cost and grad function\n",
    "#   initial_params   -- the initial parameters, i.e. the bases, w and b\n",
    "#   X_train          -- the data for the training set\n",
    "#   y_train          -- the labels for the training set\n",
    "#   X_val            -- the data for the validation set\n",
    "#   y_val            -- the labels for the validation set\n",
    "#   size_minibatches -- the size of minibatches to be used, default: 10\n",
    "#   learning_rate    -- the learning rate to update the weight matrix, default: 1\n",
    "#   max_iter         -- the maximum number of iterations to consider, default : 2000\n",
    "#\n",
    "def train_sgd(cost_and_grad, initial_params, X_train, y_train, X_val, y_val, size_minibatches=10, learning_rate=1, max_iter=2000):\n",
    "    \n",
    "    param_buf, unpack = autograd.misc.flatten(initial_params)\n",
    "    \n",
    "    # history arrays to be used to plot values\n",
    "    training_loss_history = []\n",
    "    valid_accuracy_history = []\n",
    "    num_epochs = []\n",
    "    \n",
    "    # previous validation accruacy value to be used for early stopping\n",
    "    prev_valid_accuracy = 0\n",
    "    # patience number for early stopping, tracks number of iterations the validation accuracy stayed the same for\n",
    "    patience = 0\n",
    "    \n",
    "    # the number of minibatches to be used\n",
    "    num_minibatches = int(X_train.shape[0] / size_minibatches)\n",
    "    \n",
    "    # we go over max_iter number of epochs\n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        # shuffle the sets\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        # split the sets into minibatches\n",
    "        X_train_split, y_train_split = np.array_split(X_train, num_minibatches), np.array_split(y_train, num_minibatches)\n",
    "        \n",
    "        # value to store estimate of the training loss for current epoch\n",
    "        curr_training_loss = 0\n",
    "        \n",
    "        # iterate over each minibatch and update parameters loss and param_buf\n",
    "        for minibatch in range(num_minibatches):\n",
    "            loss, dparam_buf = cost_and_grad(param_buf, X_train_split[minibatch], y_train_split[minibatch], unpack)\n",
    "            curr_training_loss += loss\n",
    "            param_buf -= learning_rate * dparam_buf\n",
    "        \n",
    "        # estimate overall training_loss by taking average over the batches\n",
    "        curr_training_loss /= num_minibatches\n",
    "        \n",
    "        # use current epoch trained model on validation set\n",
    "        y_hat = adaptive_rbfn_logits(X_val, unpack(param_buf)).argmax(axis=1)\n",
    "        \n",
    "        # if first iteration simply update prev_valid_accuracy, else do early stopping\n",
    "        if i == 0:\n",
    "            prev_valid_accuracy = np.equal(y_hat, y_val).sum() / val_size\n",
    "        else:\n",
    "            # compute new validation accuracy\n",
    "            new_valid_accuracy = np.equal(y_hat, y_val).sum() / val_size\n",
    "            # if better than previous then update previous and reset patience\n",
    "            if new_valid_accuracy > prev_valid_accuracy:\n",
    "                prev_valid_accuracy = new_valid_accuracy\n",
    "                patience = 0\n",
    "            else:\n",
    "                # if it has been too many iterations (i.e. 20) exit loop, else update patience\n",
    "                if (patience > 20):\n",
    "                    print(\"Epoch #\", i, \"-- Training loss:\", curr_training_loss, \"-- Validation accuracy:\", prev_valid_accuracy)\n",
    "                    break\n",
    "                else:\n",
    "                    patience += 1\n",
    "                    \n",
    "        # debugging statement below, uncomment to observe the results of each epoch           \n",
    "        # print(\"Epoch #\", i, \"-- Training loss:\", curr_training_loss, \"-- Validation accuracy:\", prev_valid_accuracy)\n",
    "        \n",
    "        # update history arrays\n",
    "        training_loss_history.append(curr_training_loss)\n",
    "        valid_accuracy_history.append(prev_valid_accuracy)\n",
    "        num_epochs.append(i)\n",
    "        \n",
    "    # return the results as a tuple of the learned model and the history arrays        \n",
    "    return (unpack(param_buf), training_loss_history, valid_accuracy_history, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 134 -- Training loss: 0.29656937084429325 -- Validation accuracy: 0.9272\n",
      "CPU times: user 1h 49min 12s, sys: 2min 31s, total: 1h 51min 44s\n",
      "Wall time: 1h 51min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_1 = train_sgd(adaptive_rbfn_cost_and_grad, params, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results from results_1 (minibatch size = 10)\n",
    "fig, ax1 = plot.subplots()\n",
    "ax1.plot(results_1[1], 'b-', label='Training Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Training Loss', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(results_1[2], 'r-', label='Validation Accuracy')\n",
    "ax2.set_ylabel('Validation Accuracy', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy when minibatch size = 10 is: 0.9042\n"
     ]
    }
   ],
   "source": [
    "# report the test accuracy for results_1 (minibatch size = 10)\n",
    "param_buf, unpack = autograd.misc.flatten(results_1[0])\n",
    "y_hat = adaptive_rbfn_logits(test_data, unpack(param_buf)).argmax(axis=1)\n",
    "test_accuracy = np.equal(y_hat, test_labels).sum() / (test_labels.shape[0])\n",
    "print(\"The test accuracy when minibatch size = 10 is:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 245 -- Training loss: 0.30006747027315717 -- Validation accuracy: 0.923\n",
      "CPU times: user 2h 17min 33s, sys: 15min 35s, total: 2h 33min 8s\n",
      "Wall time: 4h 40min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_2 = train_sgd(adaptive_rbfn_cost_and_grad, params, X_train, y_train, X_val, y_val, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the results from results_2 (minibatch size = 100)\n",
    "fig, ax1 = plot.subplots()\n",
    "ax1.plot(results_2[1], 'b-', label='Training Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Training Loss', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(results_2[2], 'r-', label='Validation Accuracy')\n",
    "ax2.set_ylabel('Validation Accuracy', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy when minibatch size = 100 is: 0.9177\n"
     ]
    }
   ],
   "source": [
    "# report the test accuracy for results_2 (minibatch size = 100)\n",
    "param_buf, unpack = autograd.misc.flatten(results_2[0])\n",
    "y_hat = adaptive_rbfn_logits(test_data, unpack(param_buf)).argmax(axis=1)\n",
    "test_accuracy = np.equal(y_hat, test_labels).sum() / (test_labels.shape[0])\n",
    "print(\"The test accuracy when minibatch size = 100 is:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, here is our training loss and validation accuracy curve running for 2000 epochs.  Your training and validation curve does *not* need to look like ours; this is only supposed to give you a rough idea of how the curves would look like.\n",
    "\n",
    "![](curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 5: k-NN Classifier (15 points)\n",
    "\n",
    "The second goal is to compare you ARBFN against a k-NN classifier.  For the k-NN classifier, you need to try a couple $k$'s and pick the best one in a programmatic way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "(15 points) Build a k-NN classifier using `scikit-learn` with $k$'s from 1 to 3, train it on the first 1000 examples of MNIST dataset, and report the accuracy on the full test set. <br/>\n",
    "\n",
    "Would you use the (exact) k-NN classifier you have built for very large dataset (with maybe millions of examples)?  Explain your answer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy for knn when k = 1 is:  0.869\n",
      "Testing accuracy for knn when k = 2 is:  0.8496\n",
      "Testing accuracy for knn when k = 3 is:  0.8622\n",
      "The best k for knn is k = 1 with testing accuracy:  0.869\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# flatten the training set so that it can work with sklearn\n",
    "X_train, y_train = train_data[:1000].reshape(1000, 28*28), train_labels[:1000]\n",
    "# flatten the testing set so that it can work with sklearn    \n",
    "X_test, y_test = test_data.reshape(test_data.shape[0], 28*28), test_labels\n",
    "\n",
    "# optimal k variable\n",
    "best_k = 1\n",
    "best_score = 0\n",
    "\n",
    "for k in range(1, 4):\n",
    "    # let sklearn pick the best algorithm and use Euclidean distance as distance metric (p = 2)\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, algorithm='auto', p=2)\n",
    "    knn.fit(X_train, y_train)\n",
    "    test_score = knn.score(X_test, y_test)\n",
    "    print(\"Testing accuracy for knn when k =\", k, \"is: \", test_score)\n",
    "    if test_score > best_score:\n",
    "        best_k = k\n",
    "        best_score = test_score\n",
    "\n",
    "print(\"The best k for knn is k =\", best_k, \"with testing accuracy: \", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** We see that the best testing accuracy is obtained when $k = 1$ from our k-NN models trained on the first 1000 examples of the MNIST dataset. I would most likely not use the k-NN classifier I have built with $k = 1$ as I except the accuracy of my model to decrease as the amount of testing samples increase. This is because I firstly estimate that we will need a larger $k$ when the number of samples increases due to the increased possibility of obtaining slanted or distorted samples that may be misclassified with only one near neighbor. Secondly, if $1000$ training examples leads to $0.869$ testing accuracy for a test set with $10000$ samples, it is most likely that increasing the test set size will mean that our model will not scale well due to a possibly changing distribution of the samples of the new testing data. However, if the new very large testing set shares similar distributions and samples to our current training and testing sets, then using this model will be appropriate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
